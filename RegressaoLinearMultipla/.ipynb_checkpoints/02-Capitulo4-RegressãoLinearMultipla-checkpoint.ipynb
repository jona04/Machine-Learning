{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Machine Learning</font>\n",
    "\n",
    "# <font color='blue'>Capítulo 4 - Regressão Linear Múltipla</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****** Este Jupyter Notebook foi atualizado para a versão 3.6.1. da Linguagem Python em 05/07/2017 ******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o Dataset Boston Houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CRIM: per capita crime rate by town \n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "3. INDUS: proportion of non-residential acres per town \n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "5. NOX: nitric oxides concentration (parts per 10 million) \n",
    "6. RM: average number of rooms per dwelling \n",
    "7. AGE: proportion of owner-occupied units built prior to 1940 \n",
    "8. DIS: weighted distances to five Boston employment centres \n",
    "9. RAD: index of accessibility to radial highways \n",
    "10. TAX: full-value property-tax rate per 10,000 \n",
    "11. PTRATIO: pupil-teacher ratio by town \n",
    "12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "13. LSTAT: % lower status of the population \n",
    "14. TARGET: Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando o dataset\n",
    "boston = load_boston() \n",
    "dataset = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "dataset['target'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': '/Users/jonatasilva/Documents/ProjetosPython/python/venv37/lib/python3.7/site-packages/sklearn/datasets/data/boston_house_prices.csv'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gerando número de observações e variáveis\n",
    "observations = len(dataset)\n",
    "variables = dataset.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Coletando x e y\n",
    "X = dataset.iloc[:,:-1]\n",
    "y = dataset['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,\n",
       "        18.9,  15. ,  18.9,  21.7,  20.4,  18.2,  19.9,  23.1,  17.5,\n",
       "        20.2,  18.2,  13.6,  19.6,  15.2,  14.5,  15.6,  13.9,  16.6,\n",
       "        14.8,  18.4,  21. ,  12.7,  14.5,  13.2,  13.1,  13.5,  18.9,\n",
       "        20. ,  21. ,  24.7,  30.8,  34.9,  26.6,  25.3,  24.7,  21.2,\n",
       "        19.3,  20. ,  16.6,  14.4,  19.4,  19.7,  20.5,  25. ,  23.4,\n",
       "        18.9,  35.4,  24.7,  31.6,  23.3,  19.6,  18.7,  16. ,  22.2,\n",
       "        25. ,  33. ,  23.5,  19.4,  22. ,  17.4,  20.9,  24.2,  21.7,\n",
       "        22.8,  23.4,  24.1,  21.4,  20. ,  20.8,  21.2,  20.3,  28. ,\n",
       "        23.9,  24.8,  22.9,  23.9,  26.6,  22.5,  22.2,  23.6,  28.7,\n",
       "        22.6,  22. ,  22.9,  25. ,  20.6,  28.4,  21.4,  38.7,  43.8,\n",
       "        33.2,  27.5,  26.5,  18.6,  19.3,  20.1,  19.5,  19.5,  20.4,\n",
       "        19.8,  19.4,  21.7,  22.8,  18.8,  18.7,  18.5,  18.3,  21.2,\n",
       "        19.2,  20.4,  19.3,  22. ,  20.3,  20.5,  17.3,  18.8,  21.4,\n",
       "        15.7,  16.2,  18. ,  14.3,  19.2,  19.6,  23. ,  18.4,  15.6,\n",
       "        18.1,  17.4,  17.1,  13.3,  17.8,  14. ,  14.4,  13.4,  15.6,\n",
       "        11.8,  13.8,  15.6,  14.6,  17.8,  15.4,  21.5,  19.6,  15.3,\n",
       "        19.4,  17. ,  15.6,  13.1,  41.3,  24.3,  23.3,  27. ,  50. ,\n",
       "        50. ,  50. ,  22.7,  25. ,  50. ,  23.8,  23.8,  22.3,  17.4,\n",
       "        19.1,  23.1,  23.6,  22.6,  29.4,  23.2,  24.6,  29.9,  37.2,\n",
       "        39.8,  36.2,  37.9,  32.5,  26.4,  29.6,  50. ,  32. ,  29.8,\n",
       "        34.9,  37. ,  30.5,  36.4,  31.1,  29.1,  50. ,  33.3,  30.3,\n",
       "        34.6,  34.9,  32.9,  24.1,  42.3,  48.5,  50. ,  22.6,  24.4,\n",
       "        22.5,  24.4,  20. ,  21.7,  19.3,  22.4,  28.1,  23.7,  25. ,\n",
       "        23.3,  28.7,  21.5,  23. ,  26.7,  21.7,  27.5,  30.1,  44.8,\n",
       "        50. ,  37.6,  31.6,  46.7,  31.5,  24.3,  31.7,  41.7,  48.3,\n",
       "        29. ,  24. ,  25.1,  31.5,  23.7,  23.3,  22. ,  20.1,  22.2,\n",
       "        23.7,  17.6,  18.5,  24.3,  20.5,  24.5,  26.2,  24.4,  24.8,\n",
       "        29.6,  42.8,  21.9,  20.9,  44. ,  50. ,  36. ,  30.1,  33.8,\n",
       "        43.1,  48.8,  31. ,  36.5,  22.8,  30.7,  50. ,  43.5,  20.7,\n",
       "        21.1,  25.2,  24.4,  35.2,  32.4,  32. ,  33.2,  33.1,  29.1,\n",
       "        35.1,  45.4,  35.4,  46. ,  50. ,  32.2,  22. ,  20.1,  23.2,\n",
       "        22.3,  24.8,  28.5,  37.3,  27.9,  23.9,  21.7,  28.6,  27.1,\n",
       "        20.3,  22.5,  29. ,  24.8,  22. ,  26.4,  33.1,  36.1,  28.4,\n",
       "        33.4,  28.2,  22.8,  20.3,  16.1,  22.1,  19.4,  21.6,  23.8,\n",
       "        16.2,  17.8,  19.8,  23.1,  21. ,  23.8,  23.1,  20.4,  18.5,\n",
       "        25. ,  24.6,  23. ,  22.2,  19.3,  22.6,  19.8,  17.1,  19.4,\n",
       "        22.2,  20.7,  21.1,  19.5,  18.5,  20.6,  19. ,  18.7,  32.7,\n",
       "        16.5,  23.9,  31.2,  17.5,  17.2,  23.1,  24.5,  26.6,  22.9,\n",
       "        24.1,  18.6,  30.1,  18.2,  20.6,  17.8,  21.7,  22.7,  22.6,\n",
       "        25. ,  19.9,  20.8,  16.8,  21.9,  27.5,  21.9,  23.1,  50. ,\n",
       "        50. ,  50. ,  50. ,  50. ,  13.8,  13.8,  15. ,  13.9,  13.3,\n",
       "        13.1,  10.2,  10.4,  10.9,  11.3,  12.3,   8.8,   7.2,  10.5,\n",
       "         7.4,  10.2,  11.5,  15.1,  23.2,   9.7,  13.8,  12.7,  13.1,\n",
       "        12.5,   8.5,   5. ,   6.3,   5.6,   7.2,  12.1,   8.3,   8.5,\n",
       "         5. ,  11.9,  27.9,  17.2,  27.5,  15. ,  17.2,  17.9,  16.3,\n",
       "         7. ,   7.2,   7.5,  10.4,   8.8,   8.4,  16.7,  14.2,  20.8,\n",
       "        13.4,  11.7,   8.3,  10.2,  10.9,  11. ,   9.5,  14.5,  14.1,\n",
       "        16.1,  14.3,  11.7,  13.4,   9.6,   8.7,   8.4,  12.8,  10.5,\n",
       "        17.1,  18.4,  15.4,  10.8,  11.8,  14.9,  12.6,  14.1,  13. ,\n",
       "        13.4,  15.2,  16.1,  17.8,  14.9,  14.1,  12.7,  13.5,  14.9,\n",
       "        20. ,  16.4,  17.7,  19.5,  20.2,  21.4,  19.9,  19. ,  19.1,\n",
       "        19.1,  20.1,  19.9,  19.6,  23.2,  29.8,  13.8,  13.3,  16.7,\n",
       "        12. ,  14.6,  21.4,  23. ,  23.7,  25. ,  21.8,  20.6,  21.2,\n",
       "        19.1,  20.6,  15.2,   7. ,   8.1,  13.6,  20.1,  21.8,  24.5,\n",
       "        23.1,  19.7,  18.3,  21.2,  17.5,  16.8,  22.4,  20.6,  23.9,\n",
       "        22. ,  11.9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Múltiplos Atributos com StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xc = sm.add_constant(X)\n",
    "modelo_v1 = sm.OLS(y, Xc)\n",
    "modelo_v2 = modelo_v1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.741</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.734</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   108.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 09 Jul 2017</td> <th>  Prob (F-statistic):</th> <td>6.95e-135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:25:21</td>     <th>  Log-Likelihood:    </th> <td> -1498.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3026.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   492</td>      <th>  BIC:               </th> <td>   3085.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>   36.4911</td> <td>    5.104</td> <td>    7.149</td> <td> 0.000</td> <td>   26.462</td> <td>   46.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRIM</th>    <td>   -0.1072</td> <td>    0.033</td> <td>   -3.276</td> <td> 0.001</td> <td>   -0.171</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>      <td>    0.0464</td> <td>    0.014</td> <td>    3.380</td> <td> 0.001</td> <td>    0.019</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>INDUS</th>   <td>    0.0209</td> <td>    0.061</td> <td>    0.339</td> <td> 0.735</td> <td>   -0.100</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>    <td>    2.6886</td> <td>    0.862</td> <td>    3.120</td> <td> 0.002</td> <td>    0.996</td> <td>    4.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NOX</th>     <td>  -17.7958</td> <td>    3.821</td> <td>   -4.658</td> <td> 0.000</td> <td>  -25.302</td> <td>  -10.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>      <td>    3.8048</td> <td>    0.418</td> <td>    9.102</td> <td> 0.000</td> <td>    2.983</td> <td>    4.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>     <td>    0.0008</td> <td>    0.013</td> <td>    0.057</td> <td> 0.955</td> <td>   -0.025</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>     <td>   -1.4758</td> <td>    0.199</td> <td>   -7.398</td> <td> 0.000</td> <td>   -1.868</td> <td>   -1.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RAD</th>     <td>    0.3057</td> <td>    0.066</td> <td>    4.608</td> <td> 0.000</td> <td>    0.175</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>     <td>   -0.0123</td> <td>    0.004</td> <td>   -3.278</td> <td> 0.001</td> <td>   -0.020</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PTRATIO</th> <td>   -0.9535</td> <td>    0.131</td> <td>   -7.287</td> <td> 0.000</td> <td>   -1.211</td> <td>   -0.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>       <td>    0.0094</td> <td>    0.003</td> <td>    3.500</td> <td> 0.001</td> <td>    0.004</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LSTAT</th>   <td>   -0.5255</td> <td>    0.051</td> <td>  -10.366</td> <td> 0.000</td> <td>   -0.625</td> <td>   -0.426</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>178.029</td> <th>  Durbin-Watson:     </th> <td>   1.078</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 782.015</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.521</td>  <th>  Prob(JB):          </th> <td>1.54e-170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.276</td>  <th>  Cond. No.          </th> <td>1.51e+04</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.741\n",
       "Model:                            OLS   Adj. R-squared:                  0.734\n",
       "Method:                 Least Squares   F-statistic:                     108.1\n",
       "Date:                Sun, 09 Jul 2017   Prob (F-statistic):          6.95e-135\n",
       "Time:                        22:25:21   Log-Likelihood:                -1498.8\n",
       "No. Observations:                 506   AIC:                             3026.\n",
       "Df Residuals:                     492   BIC:                             3085.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         36.4911      5.104      7.149      0.000      26.462      46.520\n",
       "CRIM          -0.1072      0.033     -3.276      0.001      -0.171      -0.043\n",
       "ZN             0.0464      0.014      3.380      0.001       0.019       0.073\n",
       "INDUS          0.0209      0.061      0.339      0.735      -0.100       0.142\n",
       "CHAS           2.6886      0.862      3.120      0.002       0.996       4.381\n",
       "NOX          -17.7958      3.821     -4.658      0.000     -25.302     -10.289\n",
       "RM             3.8048      0.418      9.102      0.000       2.983       4.626\n",
       "AGE            0.0008      0.013      0.057      0.955      -0.025       0.027\n",
       "DIS           -1.4758      0.199     -7.398      0.000      -1.868      -1.084\n",
       "RAD            0.3057      0.066      4.608      0.000       0.175       0.436\n",
       "TAX           -0.0123      0.004     -3.278      0.001      -0.020      -0.005\n",
       "PTRATIO       -0.9535      0.131     -7.287      0.000      -1.211      -0.696\n",
       "B              0.0094      0.003      3.500      0.001       0.004       0.015\n",
       "LSTAT         -0.5255      0.051    -10.366      0.000      -0.625      -0.426\n",
       "==============================================================================\n",
       "Omnibus:                      178.029   Durbin-Watson:                   1.078\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              782.015\n",
       "Skew:                           1.521   Prob(JB):                    1.54e-170\n",
       "Kurtosis:                       8.276   Cond. No.                     1.51e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.51e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_v2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
      "CRIM     1.000000 -0.199458  0.404471 -0.055295  0.417521 -0.219940  0.350784   \n",
      "ZN      -0.199458  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
      "INDUS    0.404471 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
      "CHAS    -0.055295 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
      "NOX      0.417521 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
      "RM      -0.219940  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
      "AGE      0.350784 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
      "DIS     -0.377904  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
      "RAD      0.622029 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
      "TAX      0.579564 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
      "PTRATIO  0.288250 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
      "B       -0.377365  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
      "LSTAT    0.452220 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
      "\n",
      "              DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
      "CRIM    -0.377904  0.622029  0.579564  0.288250 -0.377365  0.452220  \n",
      "ZN       0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  \n",
      "INDUS   -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800  \n",
      "CHAS    -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  \n",
      "NOX     -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879  \n",
      "RM       0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  \n",
      "AGE     -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339  \n",
      "DIS      1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  \n",
      "RAD     -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676  \n",
      "TAX     -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993  \n",
      "PTRATIO -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044  \n",
      "B        0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  \n",
      "LSTAT   -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Gerando a matriz\n",
    "X = dataset.iloc[:,:-1]\n",
    "matriz_corr = X.corr()\n",
    "print (matriz_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando um Correlation Plot\n",
    "def visualize_correlation_matrix(data, hurdle = 0.0):\n",
    "    R = np.corrcoef(data, rowvar = 0)\n",
    "    R[np.where(np.abs(R) < hurdle)] = 0.0\n",
    "    heatmap = plt.pcolor(R, cmap = mpl.cm.coolwarm, alpha = 0.8)\n",
    "    heatmap.axes.set_frame_on(False)\n",
    "    heatmap.axes.set_yticks(np.arange(R.shape[0]) + 0.5, minor = False)\n",
    "    heatmap.axes.set_xticks(np.arange(R.shape[1]) + 0.5, minor = False)\n",
    "    heatmap.axes.set_xticklabels(variables, minor = False)\n",
    "    plt.xticks(rotation=90)\n",
    "    heatmap.axes.set_yticklabels(variables, minor = False)\n",
    "    plt.tick_params(axis = 'both', which = 'both', bottom = 'off', top = 'off', left = 'off', right = 'off') \n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEdCAYAAAAIIcBlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116ef56a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizando o Plot\n",
    "visualize_correlation_matrix(X, hurdle = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando a Multicolinearidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autovalores (Eigenvalues) e Autovetores (Eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma ainda mais automática de detectar associações multicolineares (e descobrir problemas numéricos em uma inversão de matriz) é usar autovetores. Explicados em termos simples, os autovetores são uma maneira muito inteligente de recombinar a variância entre as variáveis, criando novos recursos acumulando toda a variância compartilhada. Tal recombinação pode ser obtida usando a função NumPy linalg.eig, resultando em um vetor de autovalores (representando a quantidade de variância recombinada para cada nova variável) e autovetores (uma matriz nos dizendo como as novas variáveis se relacionam com as antigas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gerando eigenvalues e eigenvectors\n",
    "corr = np.corrcoef(X, rowvar = 0)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de extrair os autovalores, imprimimos em ordem decrescente e procuramos qualquer elemento cujo valor seja próximo de zero ou pequeno em comparação com os outros. Valores próximos a zero podem representar um problema real para equações normais e outros métodos de otimização baseados na inversão matricial. Valores pequenos representam uma fonte elevada, mas não crítica, de multicolinearidade. Se você detectar qualquer um desses valores baixos, anote a posição no vetor (lembre que os índices em Python começam por zero). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O menor valor está na posição 8. Valor buscar a posição 8 no autovetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.12265476  1.43206335  1.24116299  0.85779892  0.83456618  0.65965056\n",
      "  0.53901749  0.39654415  0.06351553  0.27743495  0.16916744  0.18616388\n",
      "  0.22025981]\n"
     ]
    }
   ],
   "source": [
    "print (eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando a posição do índice na lista de autovalores, podemos encontrar o vetor específico nos autovetores que contém as variáveis carregadas, ou seja, o nível de associação com os valores originais. No eigenvector, observamos valores nas posições de índice 2, 8 e 9, que estão realmente em destaque em termos de valor absoluto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04552843  0.08089873  0.25126664 -0.03590431 -0.04389033 -0.04580522\n",
      "  0.03870705  0.01828389  0.63337285 -0.72024335 -0.02350903  0.00485021\n",
      " -0.02477196]\n"
     ]
    }
   ],
   "source": [
    "print (eigenvectors[:,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora nós imprimimos os nomes das variáveis para saber quais contribuem mais com seus valores para construir o autovetor. Associamos o vetor de variáveis com o eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDUS RAD TAX\n"
     ]
    }
   ],
   "source": [
    "print (variables[2], variables[8], variables[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo encontrado os culpados da multicolinearidade, o que devemos fazer com essas variáveis? A remoção de algumas delas é geralmente a melhor solução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente Descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gerando os dados\n",
    "observations = len(dataset)\n",
    "variables = dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos aplicar Feature Scaling através de Padronização ou Normalização. Normalização aplica escala aos dados com intervalos entre 0 e 1. A Padronização divide a média pelo desvio padrão para obter uma unidade de variância. Vamos usar a Padronização (StandardScaler) pois nesse caso esta técnica ajusta os coeficientes e torna a superfície de erros mais \"tratável\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aplicando Padronização\n",
    "standardization = StandardScaler()\n",
    "Xst = standardization.fit_transform(X)\n",
    "original_means = standardization.mean_\n",
    "originanal_stds = standardization.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gerando X e Y\n",
    "Xst = np.column_stack((Xst,np.ones(observations)))\n",
    "y  = dataset['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def random_w( p ):\n",
    "    return np.array([np.random.normal() for j in range(p)])\n",
    "\n",
    "def hypothesis(X,w):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "def loss(X,w,y):\n",
    "    return hypothesis(X,w) - y\n",
    "\n",
    "def squared_loss(X,w,y):\n",
    "    return loss(X,w,y)**2\n",
    "\n",
    "def gradient(X,w,y):\n",
    "    gradients = list()\n",
    "    n = float(len( y ))\n",
    "    for j in range(len(w)):\n",
    "        gradients.append(np.sum(loss(X,w,y) * X[:,j]) / n)\n",
    "    return gradients\n",
    "\n",
    "def update(X,w,y, alpha = 0.01):\n",
    "    return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]\n",
    "\n",
    "def optimize(X,y, alpha = 0.01, eta = 10**-12, iterations = 1000):\n",
    "    w = random_w(X.shape[1])\n",
    "    path = list()\n",
    "    for k in range(iterations):\n",
    "        SSL = np.sum(squared_loss(X,w,y))\n",
    "        new_w = update(X,w,y, alpha = alpha)\n",
    "        new_SSL = np.sum(squared_loss(X,new_w,y))\n",
    "        w = new_w\n",
    "        if k>=5 and (new_SSL - SSL <= eta and new_SSL - SSL >= -eta):\n",
    "            path.append(new_SSL)\n",
    "            return w, path\n",
    "        if k % (iterations / 20) == 0:\n",
    "            path.append(new_SSL)\n",
    "    return w, path                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes finais padronizados: -0.9204, 1.0810, 0.1430, 0.6822, -2.0601, 2.6706, 0.0211, -3.1044, 2.6588, -2.0759, -2.0622, 0.8566, -3.7487, 22.5328\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo o resultado                           \n",
    "alpha = 0.01\n",
    "w, path = optimize(Xst, y, alpha, eta = 10**-12, iterations = 20000)\n",
    "print (\"Coeficientes finais padronizados: \" + ', '.join(map(lambda x: \"%0.4f\" % x, w)))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Desfazendo a Padronização\n",
    "unstandardized_betas = w[:-1] / originanal_stds\n",
    "unstandardized_bias  = w[-1]-np.sum((original_means / originanal_stds) * w[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bias:  36.4911\n",
      "    CRIM:  -0.1072\n",
      "      ZN:   0.0464\n",
      "   INDUS:   0.0209\n",
      "    CHAS:   2.6886\n",
      "     NOX: -17.7958\n",
      "      RM:   3.8048\n",
      "     AGE:   0.0008\n",
      "     DIS:  -1.4758\n",
      "     RAD:   0.3057\n",
      "     TAX:  -0.0123\n",
      " PTRATIO:  -0.9535\n",
      "       B:   0.0094\n",
      "   LSTAT:  -0.5255\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo o resultado\n",
    "print ('%8s: %8.4f' % ('bias', unstandardized_bias))\n",
    "for beta,varname in zip(unstandardized_betas, variables):\n",
    "    print ('%8s: %8.4f' % (varname, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importância dos Atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando um modelo\n",
    "modelo = linear_model.LinearRegression(normalize = False, fit_intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.796 NOX\n",
      " 3.805 RM\n",
      " 2.689 CHAS\n",
      " 1.476 DIS\n",
      " 0.953 PTRATIO\n",
      " 0.525 LSTAT\n",
      " 0.306 RAD\n",
      " 0.107 CRIM\n",
      " 0.046 ZN\n",
      " 0.021 INDUS\n",
      " 0.012 TAX\n",
      " 0.009 B\n",
      " 0.001 AGE\n"
     ]
    }
   ],
   "source": [
    "modelo.fit(X,y)\n",
    "for coef, var in sorted(zip(map(abs, modelo.coef_), dataset.columns[:-1]), reverse = True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standardization = StandardScaler()\n",
    "Stand_coef_linear_reg = make_pipeline(standardization, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3.749 LSTAT\n",
      " 3.104 DIS\n",
      " 2.671 RM\n",
      " 2.659 RAD\n",
      " 2.076 TAX\n",
      " 2.062 PTRATIO\n",
      " 2.060 NOX\n",
      " 1.081 ZN\n",
      " 0.920 CRIM\n",
      " 0.857 B\n",
      " 0.682 CHAS\n",
      " 0.143 INDUS\n",
      " 0.021 AGE\n"
     ]
    }
   ],
   "source": [
    "Stand_coef_linear_reg.fit(X,y)\n",
    "for coef, var in sorted(zip(map(abs, Stand_coef_linear_reg.steps[1][1].coef_), dataset.columns[:-1]), reverse = True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o R Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelo = linear_model.LinearRegression(normalize = False, fit_intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def r2_est(X,y):\n",
    "    return r2_score(y, modelo.fit(X,y).predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline R2: 0.741\n"
     ]
    }
   ],
   "source": [
    "print ('Baseline R2: %0.3f' %  r2_est(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.057 LSTAT\n",
      " 0.044 RM\n",
      " 0.029 DIS\n",
      " 0.028 PTRATIO\n",
      " 0.011 NOX\n",
      " 0.011 RAD\n",
      " 0.006 B\n",
      " 0.006 ZN\n",
      " 0.006 TAX\n",
      " 0.006 CRIM\n",
      " 0.005 CHAS\n",
      " 0.000 INDUS\n",
      " 0.000 AGE\n"
     ]
    }
   ],
   "source": [
    "# Gera o impacto de cada atributo no R2\n",
    "r2_impact = list()\n",
    "for j in range(X.shape[1]):\n",
    "    selection = [i for i in range(X.shape[1]) if i!=j]\n",
    "    r2_impact.append(((r2_est(X,y) - r2_est(X.values[:,selection],y)), dataset.columns[j]))\n",
    "    \n",
    "for imp, varname in sorted(r2_impact, reverse = True):\n",
    "    print ('%6.3f %s' %  (imp, varname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obrigado - Data Science Academy - <a href=http://facebook.com/dsacademy>facebook.com/dsacademybr</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
